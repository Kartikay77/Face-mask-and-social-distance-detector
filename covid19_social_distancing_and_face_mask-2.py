# -*- coding: utf-8 -*-
"""CoVID19 SOCIAL DISTANCING AND FACE MASK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oqxh3teV-KFKzXiVYgRjX0KoIV_cwkl1
"""

!pip -q install ultralytics opencv-python numpy tqdm

!pip -q install ultralytics opencv-python numpy tqdm tensorflow

import os
import math
from typing import List, Tuple, Optional

import cv2
import numpy as np
from tqdm import tqdm

try:
    from ultralytics import YOLO
except Exception as e:
    YOLO = None

from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import img_to_array

BOX = Tuple[int, int, int, int]
Point = Tuple[float, float]


def _center_of(box: BOX) -> Point:
    x1, y1, x2, y2 = box
    return ((x1 + x2) / 2.0, (y1 + y2) / 2.0)


def _euclidean(p1: Point, p2: Point) -> float:
    return math.hypot(p1[0] - p2[0], p1[1] - p2[1])


class SocialDistanceMaskMonitor:
    # This class wraps around YOLOv8 and a mask classifier to show violations and mask usage.
    # Could probably split it into smaller components later if needed.

    def __init__(
        self,
        model_name='yolov8n.pt',
        conf=0.25,
        distance_mode='pixel',
        pixel_thresh=75.0,
        mask_model_path=None
    ):
        if YOLO is None:
            raise RuntimeError("Ultralytics not installed. Try running: pip install ultralytics")

        # Load YOLO model
        self.model = YOLO(model_name)
        self.conf = conf
        self.distance_mode = distance_mode
        self.pixel_thresh = pixel_thresh

        # Load mask model if path provided (optional)
        self.mask_model = None
        if mask_model_path and os.path.isfile(mask_model_path):
            self.mask_model = load_model(mask_model_path)

    def _detect_people(self, frame):
        results = self.model.predict(source=frame, imgsz=640, conf=self.conf, verbose=False)[0]
        people_boxes = []
        for b in results.boxes:
            if int(b.cls.item()) == 0:  # 0 = person class
                coords = b.xyxy[0].tolist()
                people_boxes.append(tuple(map(int, coords)))
        return people_boxes

    def _compute_violations(self, centers):
        violations = []
        for i in range(len(centers)):
            for j in range(i + 1, len(centers)):
                dist = _euclidean(centers[i], centers[j])
                if dist < self.pixel_thresh:
                    violations.append((i, j))
        return violations

    def _predict_mask(self, frame, box):
        if self.mask_model is None:
            return "unknown"
        x1, y1, x2, y2 = box
        face_crop = frame[y1:y2, x1:x2]
        if face_crop.size == 0:
            return "unknown"

        # Resize + normalize face for prediction
        face_resized = cv2.resize(face_crop, (128, 128))
        face_arr = img_to_array(face_resized.astype("float") / 255.0)
        face_arr = np.expand_dims(face_arr, axis=0)
        prediction = self.mask_model.predict(face_arr)[0]

        return "mask" if prediction[0] > 0.5 else "no-mask"

    def annotate_frame(self, frame):
        people_boxes = self._detect_people(frame)
        centers = [_center_of(b) for b in people_boxes]
        violators = self._compute_violations(centers)

        risky = [False] * len(centers)
        for i, j in violators:
            risky[i] = True
            risky[j] = True

        # Draw violation lines
        for i, j in violators:
            pt1 = tuple(map(int, centers[i]))
            pt2 = tuple(map(int, centers[j]))
            cv2.line(frame, pt1, pt2, (0, 0, 255), 2)  # red

        masked = 0
        nomasked = 0
        for idx, box in enumerate(people_boxes):
            x1, y1, x2, y2 = box
            mask_status = self._predict_mask(frame, box)
            if mask_status == "mask":
                masked += 1
            elif mask_status == "no-mask":
                nomasked += 1

            color = (0, 0, 255) if risky[idx] else (0, 255, 0)
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            cv2.putText(frame, mask_status, (x1, max(0, y1 - 8)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        summary = f"People: {len(centers)} | Safe: {len(centers) - sum(risky)} | Violations: {len(violators)} | Masked: {masked} | NoMask: {nomasked}"
        cv2.rectangle(frame, (10, 10), (10 + 720, 40), (0, 0, 0), -1)
        cv2.putText(frame, summary, (18, 34), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        return frame

    def process_image(self, path, out_path="output.jpg"):
        img = cv2.imread(path)
        result = self.annotate_frame(img)
        cv2.imwrite(out_path, result)

    def process_video(self, path, out_path="output.mp4"):
        cap = cv2.VideoCapture(path)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(out_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            out.write(self.annotate_frame(frame))

        cap.release()
        out.release()

    def process_webcam(self, cam_index=0):
        cap = cv2.VideoCapture(cam_index)
        if not cap.isOpened():
            raise RuntimeError("Webcam not accessible. If you're in Colab, try record_webcam().")

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                annotated = self.annotate_frame(frame)
                cv2.imshow("Webcam - Press Q to quit", annotated)
                if cv2.waitKey(1) & 0xFF in (ord("q"), ord("Q")):
                    break
        finally:
            cap.release()
            cv2.destroyAllWindows()

import cv2
import numpy as np
import os

class MaskClassifier:
    def __init__(self, mode='heuristic', keras_model_path=None):
        self.mode = mode
        self.model = None

        if self.mode == 'keras':
            # Load the .h5 model if path is provided and valid
            import tensorflow as tf
            if not keras_model_path or not os.path.exists(keras_model_path):
                raise FileNotFoundError("Keras mode enabled but no valid model file provided.")
            self.model = tf.keras.models.load_model(keras_model_path)

    @staticmethod
    def _skin_mask_bgr(bgr_image):
        # Convert to YCrCb to detect skin-like areas
        ycrcb = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2YCrCb)
        lower = np.array([0, 133, 77], dtype=np.uint8)
        upper = np.array([255, 173, 127], dtype=np.uint8)
        skin_mask = cv2.inRange(ycrcb, lower, upper)
        return cv2.medianBlur(skin_mask, 5)  # smooth mask

    def predict(self, face_img):
        if face_img is None or face_img.size == 0:
            return 0.0  # Nothing to predict

        if self.mode == 'keras' and self.model is not None:
            face_resized = cv2.resize(face_img, (128, 128))
            normalized = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
            prediction = self.model.predict(normalized[None, ...], verbose=0)[0]
            return float(prediction[0]) if prediction.shape[-1] == 2 else float(prediction.squeeze())

        # Otherwise, go heuristic — look for skin vs mask-blue ratio
        h, w = face_img.shape[:2]
        y1, y2 = h // 2, h
        x1, x2 = int(w * 0.2), int(w * 0.8)
        roi = face_img[y1:y2, x1:x2]  # lower-center region

        if roi.size == 0:
            return 0.0

        skin_mask = self._skin_mask_bgr(roi)
        skin_ratio = (skin_mask > 0).mean()

        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        blue_mask = cv2.inRange(hsv, (90, 40, 40), (130, 255, 255))
        blue_ratio = (blue_mask > 0).mean()

        is_masked = skin_ratio < 0.10 or blue_ratio > 0.15
        return 1.0 if is_masked else 0.0

class SocialDistanceMonitorV2(SocialDistanceMaskMonitor):
    def __init__(self, *args, mask_classifier=None, mask_prob_thresh=0.6, **kwargs):
        super().__init__(*args, **kwargs)
        self.mask_classifier = mask_classifier
        self.mask_prob_thresh = mask_prob_thresh  # tweakable based on classifier output

    def _predict_mask(self, frame, box):
        # If we have a custom classifier, use it
        if self.mask_classifier:
            x1, y1, x2, y2 = box
            face_crop = frame[y1:y2, x1:x2]
            if face_crop.size == 0:
                return "unknown"
            prob = self.mask_classifier.predict(face_crop)
            return "mask" if prob >= self.mask_prob_thresh else "no-mask"

        # fallback to original model logic (h5-based)
        return super()._predict_mask(frame, box)

from google.colab import output
from IPython.display import display, Javascript

def record_webcam(filename="webcam_clip.mp4", duration=5):
    js = Javascript(f"""
    async function recordVideo() {{
      const stream = await navigator.mediaDevices.getUserMedia({{video:true}});
      const recorder = new MediaRecorder(stream);
      let data = [];
      recorder.ondataavailable = e => data.push(e.data);
      recorder.start();
      await new Promise(r => setTimeout(r, {duration*1000}));
      recorder.stop();
      await new Promise(r => recorder.onstop = r);
      const blob = new Blob(data, {{type: 'video/webm'}});
      const buffer = await blob.arrayBuffer();
      const uint8 = new Uint8Array(buffer);
      google.colab.kernel.invokeFunction('notebook.saveVideo', [uint8, '{filename}'], {{}});
    }}
    recordVideo();
    """)
    display(js)

def save_video(uint8, filename):
    with open(filename, "wb") as f:
        f.write(bytearray(uint8))

output.register_callback("notebook.saveVideo", save_video)

!pip -q install ultralytics opencv-python numpy tqdm tensorflow

"""#EG1 IMAGE MASK"""

from google.colab import files
uploaded = files.upload()
for filename in uploaded.keys():
    print(f"Uploaded file: {filename}")

import os
fn = next(iter(uploaded))
src = f"/content/{fn}"
root, ext = os.path.splitext(fn)
dst = f"/content/input{ext.lower()}"

# overwrite if it already exists
if os.path.exists(dst):
    os.remove(dst)
os.rename(src, dst)

print("Saved as:", dst)

# Run detection
mask_cls = MaskClassifier(mode='heuristic')
monitor = SocialDistanceMonitorV2(
    model_name='yolov8n.pt',
    pixel_thresh=75.0,
    mask_classifier=mask_cls,
    mask_prob_thresh=0.5   # slightly lower since heuristic returns 0/1
)
monitor.process_image(dst, out_path="annotated.jpg")

from IPython.display import Image
Image("annotated.jpg")

"""#EG2 IMAGE UNMASK"""

from google.colab import files
uploaded = files.upload()
for filename in uploaded.keys():
    print(f"Uploaded file: {filename}")

import os
fn = next(iter(uploaded))                 # the actual uploaded name (handles spaces and (1).png)
src = f"/content/{fn}"
root, ext = os.path.splitext(fn)
dst = f"/content/input{ext.lower()}"      # standardize the name

# overwrite if it already exists
if os.path.exists(dst):
    os.remove(dst)
os.rename(src, dst)

print("Saved as:", dst)

mask_cls = MaskClassifier(mode='heuristic')

monitor = SocialDistanceMonitorV2(
    model_name='yolov8n.pt',
    conf=0.25,
    pixel_thresh=140,
    mask_classifier=mask_cls,
    mask_prob_thresh=0.55
)

# Use the standardized upload from earlier
import glob, cv2
img_path = sorted(glob.glob('/content/input.*'))[0]
print("Using:", img_path)

monitor.process_image(img_path, out_path="annotated.jpg")

from IPython.display import Image
Image("annotated.jpg")

"""#EG3"""

from google.colab import files
uploaded = files.upload()
for filename in uploaded.keys():
    print(f"Uploaded file: {filename}")

import os
fn = next(iter(uploaded))                 # the actual uploaded name (handles spaces and (1).png)
src = f"/content/{fn}"
root, ext = os.path.splitext(fn)
dst = f"/content/input{ext.lower()}"      # standardize the name

# overwrite if it already exists
if os.path.exists(dst):
    os.remove(dst)
os.rename(src, dst)

print("Saved as:", dst)

mask_cls = MaskClassifier(mode='heuristic')

monitor = SocialDistanceMonitorV2(
    model_name='yolov8n.pt',
    conf=0.25,
    pixel_thresh=140,
    mask_classifier=mask_cls,
    mask_prob_thresh=0.55
)

# Use the standardized upload from earlier
import glob, cv2
img_path = sorted(glob.glob('/content/input.*'))[0]
print("Using:", img_path)

monitor.process_image(img_path, out_path="annotated.jpg")

from IPython.display import Image
Image("annotated.jpg")

"""#EG4"""

from google.colab import files
uploaded = files.upload()
for filename in uploaded.keys():
    print(f"Uploaded file: {filename}")

import os
fn = next(iter(uploaded))                 # the actual uploaded name (handles spaces and (1).png)
src = f"/content/{fn}"
root, ext = os.path.splitext(fn)
dst = f"/content/input{ext.lower()}"      # standardize the name

# overwrite if it already exists
if os.path.exists(dst):
    os.remove(dst)
os.rename(src, dst)

print("Saved as:", dst)

mask_cls = MaskClassifier(mode='heuristic')

monitor = SocialDistanceMonitorV2(
    model_name='yolov8n.pt',
    conf=0.25,
    pixel_thresh=140,
    mask_classifier=mask_cls,
    mask_prob_thresh=0.55
)

# Use the standardized upload from earlier
import glob, cv2
img_path = sorted(glob.glob('/content/input.*'))[0]
print("Using:", img_path)

monitor.process_image(img_path, out_path="annotated.jpg")

from IPython.display import Image
Image("annotated.jpg")

"""#WebCam (use webcam on google chrome / edge not on safari) and can see the output in safari"""

# Cell A — record from your webcam (works in Chrome/Edge)
from google.colab import output
from IPython.display import Javascript, display

def _save_video(uint8, filename):
    with open(filename, "wb") as f:
        f.write(bytearray(uint8))
    print("Saved:", filename)

output.register_callback("notebook.saveVideo", _save_video)

def record_webcam(filename="webcam_clip.webm", duration=8):
    display(Javascript(f"""
    (async () => {{
      const div = document.createElement('div');
      const p   = document.createElement('p');
      p.textContent = 'Recording {duration} s… (preview below)';
      const video = document.createElement('video');
      video.style.width = '480px';
      video.autoplay = true; video.muted = true; video.playsInline = true;
      div.appendChild(p); div.appendChild(video); document.body.appendChild(div);

      const stream = await navigator.mediaDevices.getUserMedia({{video:true, audio:false}});
      video.srcObject = stream;

      const rec = new MediaRecorder(stream, {{mimeType:'video/webm'}});
      let chunks = [];
      rec.ondataavailable = e => chunks.push(e.data);
      rec.start();

      await new Promise(r => setTimeout(r, {duration*1000}));
      await new Promise(r => (rec.onstop = r, rec.stop()));
      stream.getVideoTracks()[0].stop();

      const blob = new Blob(chunks, {{type:'video/webm'}});
      const buf = new Uint8Array(await blob.arrayBuffer());
      google.colab.kernel.invokeFunction('notebook.saveVideo', [Array.from(buf), '{filename}'], {{}});
    }})()
    """))
record_webcam("webcam_clip.webm", duration=8)

!ffmpeg -y -loglevel error -i webcam_clip.webm -vcodec libx264 -pix_fmt yuv420p webcam_clip.mp4

mask_cls = MaskClassifier(mode='heuristic')

monitor = SocialDistanceMonitorV2(
    model_name='yolov8n.pt',
    pixel_thresh=140,  # for wider frame
    mask_classifier=mask_cls,
    mask_prob_thresh=0.55
)

monitor.process_video("webcam_clip.mp4", out_path="webcam_annotated.mp4")

from IPython.display import HTML
from base64 import b64encode

with open("webcam_annotated.mp4", "rb") as f:
    video_bytes = f.read()

data_url = "data:video/mp4;base64," + b64encode(video_bytes).decode()
HTML(f'<video controls width="800" src="{data_url}"></video>')

"""#Video"""

from google.colab import files
uploaded = files.upload()
for filename in uploaded.keys():
    print(f"Uploaded file: {filename}")

import cv2
cap = cv2.VideoCapture("/content/HappyRobot.mov")
ret, first = cap.read(); cap.release()
assert ret, "Could not read first frame."
px_thresh = int(0.15 * first.shape[1])   # e.g., 15% of width
print("pixel_thresh =", px_thresh, "| frame size =", first.shape[1], "x", first.shape[0])

# Heuristic (no extra model needed) — labels "mask" / "no-mask"
mask_cls = MaskClassifier(mode='heuristic')     # or mode='keras', keras_model_path='/content/mask_classifier.h5'

monitor = SocialDistanceMonitorV2(
    model_name='yolov8n.pt',
    conf=0.35,
    pixel_thresh=px_thresh,
    mask_classifier=mask_cls,
    mask_prob_thresh=0.6
)
monitor.process_video("/content/HappyRobot.mov", out_path="/content/annotated.mp4")

from IPython.display import HTML
from base64 import b64encode
data_url = "data:video/mp4;base64," + b64encode(open("/content/annotated.mp4","rb").read()).decode()
HTML(f'<video controls width="800" src="{data_url}"></video>')